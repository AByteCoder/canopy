system_prompt: &system_prompt |
  Use the following pieces of context to answer the user question at the next messages. This context retrieved from a knowledge database and you should use only the facts from the context to answer. Always remember to include the source to the documents you used from their 'source' field in the format 'Source: $SOURCE_HERE'.
  If you don't know the answer, just say that you don't know, don't try to make up an answer, use the context.
  Don't address the context directly, but use it to answer the user question like it's your own knowledge.
query_builder_prompt: &query_builder_prompt |
  Your task is to formulate search queries for a search engine, to assist in responding to the user's question.
  You should break down complex questions into sub-queries if needed.

tokenizer:
  type: OpenAITokenizer
  params:
    model_name: gpt-3.5-turbo

chat_engine:
  params:
    max_prompt_tokens: 4096
    max_generated_tokens: null # Will use the LLM's default max generated tokens
    max_context_tokens: null   # Will use 70% of the max_prompt_tokens
    system_prompt: *system_prompt
    history_pruning: recent  # Options: [raise, recent]
    min_history_messages: 1

  llm: &llm
    type: OpenAILLM
    params:
      model_name: gpt-3.5-turbo

  query_builder:
    type: FunctionCallingQueryGenerator
    params:
      prompt: *query_builder_prompt
      function_description: Query search engine for relevant information

    llm:
      <<: *llm

  context_engine:
    params:
      global_metadata_filter: null # An optional metadata filter to apply to all queries

    knowledge_base:
      params:
        default_top_k: 5

      record_encoder:
        type: OpenAIRecordEncoder
        params:
          model_name: text-embedding-ada-002
          batch_size: 400

      chunker:
        type: MarkdownChunker
        params:
          chunk_size: 256
          chunk_overlap: 0
          keep_separator: true

    context_builder:
      type: StuffingContextBuilder
